{
 "metadata": {
  "name": "",
  "signature": "sha256:3e20d3e00998f7ae721a12ba377b6ded0ce72efa212a5a4e9e255be3ec799d0f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Cahn-Hilliard with Legendre Basis\n",
      "\n",
      "This example uses a Cahn-Hilliard model to compare two different bases representations to discretize the microstructure. One basis representaion uses the continuous indicator basis (also known as a primitive or binned basis) and the other uses Legendre polynomials. The example includes the background theory about using Legendre polynomials as a basis in MKS. The MKS with two different bases are compared with the standard spectral solution for the Cahn-Hilliard solution at both the calibration domain size and a scaled domain size. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Cahn-Hilliard Equation\n",
      "\n",
      "The Cahn-Hilliard equation is used to simulate microstructure evolution during spinodial decomposition and has the following form,\n",
      "\n",
      "$$ \\dot{\\phi} = \\nabla^2 \\left( \\phi^3 - \\phi \\right) - \\gamma \\nabla^4 \\phi $$\n",
      "\n",
      "where $\\phi$ is a conserved ordered parameter and $\\sqrt{\\gamma}$ represents the width of the interface. In this example, the Cahn-Hilliard equation is solved using a semi-implicit spectral scheme with periodic boundary conditions, see  [Chang and Rutenberg](http://dx.doi.org/10.1103/PhysRevE.72.055701) for more details."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Legendre Polynomial Basis for the Microstructure Function\n",
      "\n",
      "#### Continuous Indicator Basis\n",
      "\n",
      "Recall the previous\n",
      "[Cahn-Hilliard example](http://pymks.org/rst/cahn_hilliard_Legendre.html),\n",
      "the convolution at time $t$ is given by\n",
      "\n",
      "$$ p\\left[i, t \\right] = \\sum_{h=0}^{n-1} \\alpha_h \\left[j\\right] m_h\\left[i - j, t\\right] $$\n",
      "\n",
      "In this example the $p$ are an approximation of the updated $\\phi$\n",
      "such that,\n",
      "\n",
      "$$ p \\left[i, t\\right] \\approx \\phi \\left[i, t + \\Delta t \\right] $$\n",
      "\n",
      "The `ContinuousIndicatorBasis` uses a simple primitive or binned basis\n",
      "to discretize state space such that\n",
      "\n",
      "$$ \\phi\\left[i\\right] = \\sum_{h=0}^{n-1} m_h\\left[i\\right] \\chi_h $$\n",
      "\n",
      "dropping the superscript. In this example, $-1 \\le \\phi \\le 1$ so that\n",
      "$\\Delta h = 2 / (n - 1)$ where $n\\ge2$ is the number of spatial bins.\n",
      "\n",
      "The $\\chi_h$ are given by $-1 + h \\Delta h$. The $m_h$ can be represented by\n",
      "\n",
      "$$ m_h \\left[i\\right] = R\\left(1 - \\frac{\\left| \\phi\\left[i\\right] - \\chi_h \\right|}{\\Delta h}\\right) $$\n",
      "\n",
      "where $R$ is the\n",
      "[ramp function](http://en.wikipedia.org/wiki/Ramp_function). There is\n",
      "a mapping both ways between $\\phi$ and $m_h$. See [Fast el al.](http://dx.doi.org/10.1016/j.actamat.2010.10.008) for further details on solving the Cahn-Hilliard equation with this basis.\n",
      "\n",
      "#### Legendre Polynomial Basis\n",
      "\n",
      "The microstructure function can also be represented using the\n",
      "coefficients from a Legendre series, such that,\n",
      "\n",
      "$$ m_h\\left[i\\right] = c_h \\left[i\\right] $$\n",
      "\n",
      "where the $c_h$ are the coefficients in a Legendre series. As we shall\n",
      "see, the coefficients turn out to be Legendre polynomials\n",
      "themselves. The $m_h$ vector only needs to be a unique mapping from a\n",
      "value $\\phi$ so it is arbitrary in terms of how this is achieved. The\n",
      "following steps show how to construct the $c_h$. Start with a Legendre\n",
      "series,\n",
      "\n",
      "$$ f \\left( \\eta, \\phi \\right) = \\sum_{l = 0}^\\infty c_{l} \\left(\\phi\\right) P_l (\\eta) $$\n",
      "\n",
      "where $\\eta$ is a continuous variable such $-1 \\le \\eta \\le 1$ and\n",
      "$\\phi$ is a fixed value at a location in discretized space (the $[i]$\n",
      "is dropped for convenience). Using\n",
      "\n",
      "$$ \\int_{-1}^{1} P_l \\left( \\eta \\right)  P_h \\left( \\eta \\right) d\\xi = \\delta_{lh}\\frac{2}{2h + 1} $$\n",
      "\n",
      "and the expression for $f$ above, we can write the $c_l$ as\n",
      "\n",
      "$$ c_h \\left(\\phi\\right) = \\frac{2h +1}{2} \\int_{-1}^1 P_h \\left(\\eta \\right) f\\left( \\eta, \\phi \\right) d\\eta $$\n",
      "\n",
      "Now we choose $f$ such that\n",
      "\n",
      "$$ f \\left(\\eta, \\phi\\right) = \\delta\\left(\\phi - \\eta \\right) $$\n",
      "\n",
      "where we are conveniently assuming that $\\phi$ has the same range has\n",
      "$\\xi$ (a linear mapping would be needed if this wasn't the case). This\n",
      "choice of a $\\delta$ function is good in two ways, it allows the\n",
      "integral to be easily evaluated and it also provides a unique mapping\n",
      "between $\\phi$ and $m_h$. Using this choice, the $m_h$ can be written\n",
      "as\n",
      "\n",
      "$$ m_h \\left[i\\right] = c_h \\left[i\\right] = \\frac{2h +1}{2} P_h \\left(\\phi\\left[i\\right]\\right) $$\n",
      "\n",
      "For completeness, we can trivially construct a reverse mapping by\n",
      "observing that $P_1 \\left( x \\right) = x$ so that $\\phi = 2 m_1 / 3$.\n",
      "\n",
      "Further description is required to show mathematically why the $P_n$\n",
      "make such a good basis.\n",
      "\n",
      "In this example, we will explore the differences when using the\n",
      "Legendre polynomials as the basis function compared to the primitive\n",
      "(binned) basis for the microstructure function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Modeling with MKS\n",
      "\n",
      "###Generating Calibration Datasets\n",
      "\n",
      "Because the microstructure is a continuous field that can have a range of values and changes over time, the first order influence coefficients cannot be calibrated with delta microstructures. Instead a large number of simulations with random initial conditions will be used to calibrate the first order influence coefficients using linear regression. Let's show how this is done.\n",
      "\n",
      "The function `make_cahnHilliard` from `pymks.datasets` provides a nice interface to generate calibration datasets for the influence coefficients. The funcion `make_cahnHilliard` requires the number of calibration samples given by `n_samples` and the size and shape of the domain given by `size`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pymks\n",
      "from pymks.datasets import make_cahn_hilliard\n",
      "\n",
      "L = 41\n",
      "n_samples = 400\n",
      "dt = 1e-2\n",
      "np.random.seed(101)\n",
      "size=(L, L)\n",
      "X, y = make_cahn_hilliard(n_samples=n_samples, size=size, dt=dt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The function `make_cahnHilliard` has generated `n_samples` number of random microstructures, `X`, and returned the same microstructures after they have evolved for one time step given by `y`. Let's take a look at one of them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pymks.tools import draw_concentrations\n",
      "\n",
      "draw_concentrations(X[0], y[0], title0='time = 0', title1='time = 1')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Calibrate Influence Coefficients\n",
      " \n",
      "In this example, we compare the difference between using the \"primitive basis\" and the Legendre polynomial basis to represent the microstructure function. As mentioned above, the microstructures (concentration fields) are not discrete phases. This leaves the number of local states in local state space `n_states` as a free hyper parameter. In the next section we look to see what a practical number of bins and Legendre polynomials would be. \n",
      " \n",
      "#### Optimizing the Number of Local States\n",
      " \n",
      "Below, we compare the difference in performance as we vary the local state when we choose the binned basis and the Legendre polynomial basis.\n",
      "\n",
      "The `(X, y)` sample data is split into training and test data. The code then optimizes `n_states` between `2` and `11` and the two `basis` with the `parameters_to_tune` variable. The `GridSearchCV` takes an `MKSRegressionModel` instance, a `scoring` function (figure of merit) and the `parameters_to_tune` and then finds the optimal parameters with a grid search."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pymks.bases import ContinuousIndicatorBasis\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn import metrics\n",
      "mse = metrics.mean_squared_error\n",
      "from pymks.bases import LegendreBasis\n",
      "from pymks import MKSRegressionModel\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "train_split_shape = (X.shape[0],) + (np.prod(X.shape[1:]),)\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X.reshape(train_split_shape),\n",
      "                                                    y.reshape(train_split_shape),\n",
      "                                                    test_size=0.5, random_state=3)\n",
      "\n",
      "continuous_basis = ContinuousIndicatorBasis(2, [-1, 1])\n",
      "legendre_basis = LegendreBasis(2, [-1, 1])\n",
      "\n",
      "params_to_tune = {'n_states': np.arange(2, 11),\n",
      "                 'basis': [continuous_basis, legendre_basis]}\n",
      "model = MKSRegressionModel(continuous_basis)\n",
      "scoring = metrics.make_scorer(lambda a, b: -mse(a, b))\n",
      "fit_params = {'size': size}\n",
      "gs = GridSearchCV(model, params_to_tune, cv=5, scoring=scoring, fit_params=fit_params).fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The optimal parameters are a Legendre polynomial basis with only 4 terms. More terms don't improve the mean square error."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(gs.best_estimator_)\n",
      "print(gs.score(X_test, y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pymks.tools import draw_gridscores\n",
      "\n",
      "lgs = [x for x in gs.grid_scores_ \\\n",
      "       if type(x.parameters['basis']) is type(legendre_basis)]\n",
      "cgs = [x for x in gs.grid_scores_ \\\n",
      "       if type(x.parameters['basis']) is type(continuous_basis)]\n",
      "\n",
      "draw_gridscores(lgs, 'Legendre', '#f46d43')\n",
      "draw_gridscores(cgs, 'Continuous', '#1a9641')\n",
      "legend = plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you can see the `Legendre` basis converges faster than the binned basis. In order to further compare performance between the two models, lets select 4 local states for both bases."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Comparing the Bases for `n_states=4`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pymks import MKSRegressionModel\n",
      "\n",
      "BinBasis = ContinuousIndicatorBasis(n_states=4, domain=[-1, 1])\n",
      "BinModel = MKSRegressionModel(basis=BinBasis)\n",
      "BinModel.fit(X, y)\n",
      "\n",
      "LegendreBasis = LegendreBasis(4, [-1, 1])\n",
      "LegendreModel = MKSRegressionModel(basis=LegendreBasis)\n",
      "LegendreModel.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's look at the influence coefficients for both bases.\n",
      "\n",
      "First the binned basis influence coefficients"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pymks.tools import draw_coeff\n",
      "\n",
      "draw_coeff(BinModel.coeff)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now for the Legendre polynomial basis influence coefficients."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "draw_coeff(LegendreModel.coeff)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's do some simulations with both sets of coefficients and compare the results."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Predict Microstructure Evolution\n",
      "\n",
      "In order to compare the difference between the two bases, we need to have the Cahn-Hilliard simulation and the two MKS models start with the same initial concentration `phi0` and evolve in time. In order to do the Cahn-Hilliard simulation we need an instance of the class `CahnHilliardSimulation`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pymks.datasets.cahn_hilliard_simulation import CahnHilliardSimulation\n",
      "np.random.seed(66)\n",
      "\n",
      "phi0 = np.random.normal(0, 1e-9, ((n_samples,) + size))\n",
      "CHSim = CahnHilliardSimulation(dt=dt)\n",
      "phi = phi0.copy()\n",
      "phi_bin_pred = phi0.copy()\n",
      "phi_legendre_pred = phi0.copy()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to move forward in time, we need to feed the concentration back into the Cahn-Hilliard simulation and the MKS models."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "time_steps = 55\n",
      "\n",
      "for ii in range(time_steps):\n",
      "    CHSim.run(phi)\n",
      "    phi = CHSim.response\n",
      "    phi_bin_pred = BinModel.predict(phi_bin_pred)\n",
      "    phi_legendre_pred = LegendreModel.predict(phi_legendre_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's take a look at the concentration fields."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pymks.tools import draw_concentrations\n",
      "\n",
      "draw_concentrations(phi[0], phi_bin_pred[0], phi_legendre_pred[0], title0='Simulation', title1='Bin', title2='Legendre')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By just looking at the three microstructures is it difficult to see any differences. Below, we plot the difference between the two MKS models and the simulation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "mse = metrics.mean_squared_error\n",
      "from pymks.tools import draw_diff\n",
      "\n",
      "draw_diff((phi[0] - phi_bin_pred[0]), (phi[0] - phi_legendre_pred[0]), title0='Simulaiton - Bin', title1='Simulation - Legendre')\n",
      "print 'Bin mse =',mse(phi[0], phi_bin_pred[0])\n",
      "print 'Legendre mse =',mse(phi[0], phi_legendre_pred[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Legendre polynomial basis clearly out performs the binned basis for the same value of `n_states`."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Resizing the Coefficients to use on Larger Systems \n",
      "\n",
      "Below we compare the bases after the coefficients are resized."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = 3 * L\n",
      "BinModel.resize_coeff((N, N))\n",
      "LegendreModel.resize_coeff((N, N))\n",
      "\n",
      "phi0 = np.random.normal(0, 1e-9, (1, N, N))\n",
      "phi = phi0.copy()\n",
      "phi_bin_pred = phi0.copy()\n",
      "phi_legendre_pred = phi0.copy()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's look at the resized coefficients.\n",
      "\n",
      "First the influence coefficients from the binned bases."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "draw_coeff(BinModel.coeff)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now the influence coefficients from the Legendre polynomial bases."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "draw_coeff(LegendreModel.coeff)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once again we are going to march forward in time by feeding the concentration fields back into the Cahn-Hilliard simulation and the MKS models. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for ii in range(1000):\n",
      "    CHSim.run(phi)\n",
      "    phi = CHSim.response\n",
      "    phi_bin_pred = BinModel.predict(phi_bin_pred)\n",
      "    phi_legendre_pred = LegendreModel.predict(phi_legendre_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "draw_concentrations(phi[0], phi_bin_pred[0], phi_legendre_pred[0], title0='Simulation', title1='Bin', title2='Legendre')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Both the MKS models seem to predict the concentration faily well. However, the Legendre polynomial basis looks to be better. Again let's look at the difference between the simulation and the MKS models."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pymks.tools import draw_diff\n",
      "\n",
      "draw_diff((phi[0] - phi_bin_pred[0]), (phi[0] - phi_legendre_pred[0]), \n",
      "           title0='Simulaiton - Bin', title1='Simulation - Legendre')\n",
      "print 'Bin mse =',mse(phi[0], phi_bin_pred[0])\n",
      "print 'Legendre mse =',mse(phi[0], phi_legendre_pred[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With the resized influence coefficients, the Legendre polynomial outperforms the binned basis for the same value of `n_states`. The value of `n_states` does not necessarily guarantee a fair comparison between the two basis in terms of floating point calculations and memory used."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}