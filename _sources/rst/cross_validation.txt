
Cross Validation and Hyperparameter Optimization
================================================

In this notebook, we will

-  split the sample data into test and training sets,

-  optimize the ``Nbin`` hyperparameter,

-  learn to use `Sklearn <http://scikit-learn.org>`__ to cross validate
   the model,

-  plot learning curves as a function of training sets.

.. code:: python

    %matplotlib inline
    %load_ext autoreload
    %autoreload 2
    
    import numpy as np
    import matplotlib.pyplot as plt
    from pymks import MKSRegressionModel
    from pymks import FiPyCHModel
Make some actual data (400 samples) with a random seed of 101.

.. code:: python

    np.random.seed(101)
    X = np.random.random((400, 21, 21))
    fipymodel = FiPyCHModel()
    y = fipymodel.predict(X)
The FiPy Cahn-Hilliard model has been packaged into the ``FiPyCHModel``
class within the ``fit``, ``predict`` paradigm.

.. code:: python

    fipymodel.fit(None, None)

::


    ---------------------------------------------------------------------------
    NotImplementedError                       Traceback (most recent call last)

    <ipython-input-3-2db047ef34e7> in <module>()
    ----> 1 fipymodel.fit(None, None)
    

    /home/dbb1/git/pymks/pymks/fipyCHModel.py in fit(self, X, y)
         16 
         17     def fit(self, X, y):
    ---> 18         raise NotImplementedError
         19 
         20     def predict(self, X):


    NotImplementedError: 


A sanity check
--------------

Let's fit the data.

.. code:: python

    model = MKSRegressionModel(Nbin=10)
    model.fit(X, y)
The ``model`` now knows its coefficients

.. code:: python

    model.Fcoeff.shape



.. parsed-literal::

    (21, 21, 10)



Let's check that the fit sort of "looks right" with one test sample

.. code:: python

    np.random.seed(102)
    X_test = np.random.random((1,) + X.shape[1:])
    y_test = fipymodel.predict(X_test)
    y_pred = model.predict(X_test)
    
    np.random.seed(2)
    index = np.random.randint(len(y_test.flatten()), size=10)
    print y_test.flatten()[index]
    print y_pred.flatten()[index]

.. parsed-literal::

    [ 23950253.78282329  13425583.87216847  10370446.72304485
      -5082255.45966466 -37232615.8747914   26787885.25872415
      24692312.47186201  27598782.37350243   2548349.44343851   9095170.7858277 ]
    [ 23924081.38330388  13398981.55267725  10372689.12723125
      -5103357.55708056 -37274867.56519654  26809449.81666647
      24722045.53760799  27628866.17868118   2557932.70513816
       9110175.33127278]


and check the "mean square error" using Sklearn's ``mean_squared_error``
function.

.. code:: python

    from sklearn import metrics
    mse = metrics.mean_squared_error
    '%1.3e' % np.sqrt(mse(y_test[0], y_pred[0]))



.. parsed-literal::

    '5.984e+04'



Seems okay. How do the coefficients look?

.. code:: python

    coeff = np.fft.ifftn(model.Fcoeff, axes=(0, 1)).real
    Nroll = coeff.shape[0] / 2
    coeff_rolled = np.roll(np.roll(coeff[:,:,0], Nroll, axis=0), Nroll, axis=1)
    plt.contourf(coeff_rolled, 249)
    plt.colorbar()



.. parsed-literal::

    <matplotlib.colorbar.Colorbar instance at 0x5806680>




.. image:: cross_validation_files/cross_validation_15_1.png


The coefficients look like one would expect with the nearest neighbor
cells having the highest influence on a cell.

Training and testing
--------------------

Now, let's use the Sklearn function ``train_test_split`` to split the
data into training and test data sets. If we use the entire set of data
for the fitting and leave nothing for testing, we have no idea if we are
simply "overfitting" the data. Think of the analogy of using a high
order polynomial to fit data points on a graph, while it may fit the
data perfectly, we learn nothing as it is a useless model for fitting
subsequently generated data.

The argument ``test_size=0.5`` splits the data into equal sized chunks
for training and testing.

.. code:: python

    from sklearn.cross_validation import train_test_split
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=3)
.. code:: python

    print X_train.shape

.. parsed-literal::

    (200, 21, 21)


Let's refit with the training data set only.

.. code:: python

    model = MKSRegressionModel(Nbin=10)
    model.fit(X_train, y_train)
How well does it predict?

.. code:: python

    '%1.3e' % mse(model.predict(X_test), y_test)



.. parsed-literal::

    '4.018e+09'



The above is just one way to split the data. We may want to check with
alternative splits of the data. This is easy using Sklearn's
``cross_validation`` module. Here we do ``10`` different splits of the
entire dataset and check the mean and standard deviation of the mean
square error. To do this we first need to define a "scoring" function.

.. code:: python

    def neg_mse(a, b):
        return -mse(a, b)
    
    scoring = metrics.make_scorer(neg_mse)
Now pass the 'scoring' function as an arguement to do cross validation.

.. code:: python

    from sklearn import cross_validation
    
    model = MKSRegressionModel(Nbin=10)
    scores = cross_validation.cross_val_score(model, X, y, scoring=scoring, cv=20)
    print("MSE: %1.3e (+/- %1.3e)" % (scores.mean(), scores.std()))

.. parsed-literal::

    MSE: -3.946e+09 (+/- 2.568e+08)


The ``MKSRegressionModel`` has methods inherited from the Sklearn's
``LinearRegressionModel`` that allows the use of ``cross_validation``.

The ``LinearRegressionModel`` may not be the best class to inherit from.
Fitting MKS into Sklearn needs careful consideration.

Optimize the ``Nbin`` hyperparameter
------------------------------------

``Nbin`` is known as a hyperparameter. Hyperparameters are parameters
that influence the fitting, but are separate from the data and the
parameters used to generate the data. To demonstrate the need to
optimize hyperparameters, let look at when we use all the data (``X``
and ``y``) to call ``fit`` for various values of ``Nbin``, and calculate
the mean square error by comparing the predicted data against the
original data, ``y``.

.. code:: python

    import matplotlib.pyplot as plt
    
    mse = metrics.mean_squared_error
    
    Nbins = np.arange(2, 100, 10)
    
    errors = []
    
    for Nbin in Nbins:
        print Nbin
        model = MKSRegressionModel(Nbin=Nbin)
        model.fit(X, y)
        errors.append(mse(model.predict(X), y))
        
    plt.plot(Nbins, errors)
    plt.xlabel('Nbin')
    plt.ylabel('MSE')

.. parsed-literal::

    2
    12
    22
    32
    42
    52
    62
    72
    82
    92




.. parsed-literal::

    <matplotlib.text.Text at 0x4eee350>




.. image:: cross_validation_files/cross_validation_31_2.png


In contrast, using only the training data (``X_train``, ``y_train``) to
fit the MKS model the mean square error has a minimum value for a given
number of ``Nbin``.

.. code:: python

    errors = []
    
    Nbins = np.arange(2, 20)
    for Nbin in Nbins:
        model = MKSRegressionModel(Nbin=Nbin)
        model.fit(X_train, y_train)
        errors.append(mse(model.predict(X_test), y_test))
        
    plt.plot(Nbins, errors)
    plt.xlabel('Nbin')
    plt.ylabel('MSE')
    
    argmin = np.argmin(errors)
    print "optimal Nbin: {0}, mse: {1:1.3e}".format(Nbins[argmin], errors[argmin])

.. parsed-literal::

    optimal Nbin: 6, mse: 3.976e+09



.. image:: cross_validation_files/cross_validation_33_1.png


By seperating the data into test and predict, we are able to objectivly
optimize the hyperparameter ``Nbin``.

Training, validation and test data for hyperparameter optimization
------------------------------------------------------------------

Using the test data to optimize hyperparameters leads to test data
knowledge leaking into the model. A way to avoid this is to optimize the
hyperparamters with a validation data set separate from the test data
set. Sklearn provides ``GridSearchCV`` to automate the optimization of
hyperparamters using only the training data without using the test data.

.. code:: python

    from sklearn.grid_search import GridSearchCV
    
    parameters_to_tune = [{'Nbin': np.arange(2, 20)}]
Using the scoring function created earlier, generate a ``GridSearchCV``
instance.

.. code:: python

    gridSearch = GridSearchCV(MKSRegressionModel(Nbin=10), parameters_to_tune, cv=5, scoring=scoring)
Optimize with only the training data.

.. code:: python

    gridSearch.fit(X_train, y_train)



.. parsed-literal::

    GridSearchCV(cv=5, estimator=MKSRegressionModel(Nbin=10), fit_params={},
           iid=True, loss_func=None, n_jobs=1,
           param_grid=[{'Nbin': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,
           19])}],
           pre_dispatch='2*n_jobs', refit=True, score_func=None,
           scoring=make_scorer(neg_mse), verbose=0)



Find the best estimator.

.. code:: python

    print(gridSearch.best_estimator_)
    print gridSearch.score(X_test, y_test)

.. parsed-literal::

    MKSRegressionModel(Nbin=5)
    0.999989352422


What were the MSEs for each value of ``Nbin``.

.. code:: python

    for params, mean_score, scores in gridSearch.grid_scores_:
        print("%1.3e (+/-%1.3e) for %r"% (mean_score, scores.std() / 2, params))

.. parsed-literal::

    -4.119e+09 (+/-4.555e+07) for {'Nbin': 2}
    -4.136e+09 (+/-4.557e+07) for {'Nbin': 3}
    -3.932e+09 (+/-4.326e+07) for {'Nbin': 4}
    -3.916e+09 (+/-4.207e+07) for {'Nbin': 5}
    -3.921e+09 (+/-4.219e+07) for {'Nbin': 6}
    -3.932e+09 (+/-4.355e+07) for {'Nbin': 7}
    -3.946e+09 (+/-4.307e+07) for {'Nbin': 8}
    -3.964e+09 (+/-4.413e+07) for {'Nbin': 9}
    -3.974e+09 (+/-4.380e+07) for {'Nbin': 10}
    -3.994e+09 (+/-4.576e+07) for {'Nbin': 11}
    -4.012e+09 (+/-4.730e+07) for {'Nbin': 12}
    -4.024e+09 (+/-4.554e+07) for {'Nbin': 13}
    -4.042e+09 (+/-4.863e+07) for {'Nbin': 14}
    -4.059e+09 (+/-4.539e+07) for {'Nbin': 15}
    -4.070e+09 (+/-4.556e+07) for {'Nbin': 16}
    -4.091e+09 (+/-4.561e+07) for {'Nbin': 17}
    -4.112e+09 (+/-4.638e+07) for {'Nbin': 18}
    -4.128e+09 (+/-4.684e+07) for {'Nbin': 19}


How does that match the test data?

.. code:: python

    y_true, y_pred = y_test, gridSearch.predict(X_test)
    '%1.3e' % mse(y_true, y_pred)



.. parsed-literal::

    '3.980e+09'



What about for other values of ``Nbin``.

.. code:: python

    for Nbin in parameters_to_tune[0]['Nbin']:
        model = MKSRegressionModel(Nbin=Nbin)
        model.fit(X_train, y_train)
        y_true, y_pred = y_test, model.predict(X_test)
        print 'Nbin: {0}, mse: {1:1.3e}'.format(Nbin, mse(y_true, y_pred))

.. parsed-literal::

    Nbin: 2, mse: 4.184e+09
    Nbin: 3, mse: 4.199e+09
    Nbin: 4, mse: 3.997e+09
    Nbin: 5, mse: 3.980e+09
    Nbin: 6, mse: 3.976e+09
    Nbin: 7, mse: 3.990e+09
    Nbin: 8, mse: 4.001e+09
    Nbin: 9, mse: 4.017e+09
    Nbin: 10, mse: 4.025e+09
    Nbin: 11, mse: 4.044e+09
    Nbin: 12, mse: 4.044e+09
    Nbin: 13, mse: 4.067e+09
    Nbin: 14, mse: 4.082e+09
    Nbin: 15, mse: 4.089e+09
    Nbin: 16, mse: 4.104e+09
    Nbin: 17, mse: 4.114e+09
    Nbin: 18, mse: 4.120e+09
    Nbin: 19, mse: 4.141e+09


Learning Curves
===============

Learning curves provide some insight as to whether the number of samples
you have can represent the distribution you predicting from by plotting
a ``Training score`` curve and a ``Cross-validation score`` curve. The
``Training score`` curve plots the score of mksRegressionModel using the
entire dataset, and the ``Cross-validation score`` curve preforms the
score with a cross-validation as a function of number of samples. Let's
plot the learning curves for the for the mksRegressionModel using random
microstructures.

.. code:: python

    from pymks.tools import plot_learning_curve
    title = 'Cahn-Hillard with MKS'
    plot_learning_curve(estimator=model, title=title, X=X,\
                        y=y, ylim=(0.0, 1.1), cv=15, \
                        train_sizes=np.linspace(0.01, 0.1, 30)).show()


.. image:: cross_validation_files/cross_validation_50_0.png

