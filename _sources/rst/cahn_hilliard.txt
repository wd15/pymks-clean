
Cahn-Hilliard
=============

This example demonstrates how to use PyMKS to quickly produce phase
field results from a Cahn-Hilliard simulation. The first section
provides some background information about the Cahn-Hilliard equation as
well as details behind the Cahn-Hilliard simulation used to calibration
and validate the MKS model. The example then goes on to demonstrate how
to generate calibration data to calibrate the influence coefficients
using linear regression. As part of the calibration process, the example
explains how to pick an appropriate number of local states for the MKS
model when state space is continuous. The Cahn-Hilliard simulation and
the MKS model with calibrated influence coefficients are then used to
predict the microstructure evolution from a given initial concentration
and the results are compared. Finally, the influence coefficients are
scaled up and used to predict the evolution for a much larger system.

Cahn-Hilliard Equation
~~~~~~~~~~~~~~~~~~~~~~

The Cahn-Hilliard equation is used to simulation microstructure
evolution during spinodial decompation. The equation take the following
form.

.. math::  \frac{\partial \phi}{\partial t} = \nabla \cdot D \nabla \Bigg ( \frac{\partial f}{\partial \phi} + \epsilon^2 \nabla^2 \phi \Bigg )

:math:`\phi` is an conserved ordered parameter which can represent alloy
composition, :math:`D` is the diffusion coefficient,
:math:`f(\phi(x,t))` is the free energy function, and :math:`\epsilon`
is the length of interface region. :math:`f(Ï•(x,t))` is a double well
potential and can be represented as a fourth-order polynomial.

.. math::  f = \frac{a^2}{2}\phi^2 (1 - \phi)^2

The free paramter :math:`a` can be used to control the height of the
free energy function while the free parameter $ $ can be used to control
to interface width. The double well provides the thermodynamic driving
force for phase seperation. While the gradient energy term
:math:`\epsilon^2 \nabla^2 \phi` provides the driving force to keep the
interface between the phases continous.

In this example, a 2D concentration fields :math:`\phi` are simulated by
solving the Cahn-Hilliard equation using the spectral method with
periodic boundary conditions [1]. The simulated concentration fields are
used to both calibrate the influence coefficients and validate the
accuracy of the MKS model.

.. code:: python

    %matplotlib inline
    %load_ext autoreload
    %autoreload 2
    
    import numpy as np
    import matplotlib.pyplot as plt
Modeling with MKS
-----------------

Calibration Datasets
~~~~~~~~~~~~~~~~~~~~

Unlike the elastostatic examples, the microstructure (concentration
field) for this simulation doesn't have discrete phases. The
microstructure is a continuous field that can have a range of values
which can change over time, therefore the first order influence
coefficients cannot be calibrated with delta microstructures. Instead a
large number of simulations with random initial conditions are used to
calibrate the first order influence coefficients using linear
regression. Let's show how this is done.

The function ``make_cahnHilliard`` from ``pymks.datasets`` provides a
nice interface to generate calibration datasets for the influence
coefficients. To use ``make_cahnHilliard``, we need to set the number of
samples we want to use to calibrate the influence coefficients using
``n_samples`` as well as the size of the simulation using ``size``.

.. code:: python

    import pymks
    from pymks.datasets import make_cahnHilliard
    
    L = 40
    n_samples = 400
    dt=1e-2
    np.random.seed(99)
    X, y = make_cahnHilliard(n_samples=n_samples, size=(L, L), dt=dt)

The function ``make_cahnHilliard`` has generated ``n_samples`` number of
random microstructures ``X`` and returned the same microstructures after
they have evolved one time step ``y``. Let's take a look at one of them.

.. code:: python

    from pymks.tools import draw_concentrations
    
    draw_concentrations(X[0], y[0], title0='time=0', title1='time = 1')

.. parsed-literal::

    /home/wd15/anaconda/lib/python2.7/site-packages/matplotlib/figure.py:1595: UserWarning: This figure includes Axes that are not compatible with tight_layout, so its results might be incorrect.
      warnings.warn("This figure includes Axes that are not "



.. image:: cahn_hilliard_files/cahn_hilliard_6_1.png


Calibrate Influence Coefficients
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

As mentioned above, the microstructures (concentration fields) does not
have discrete phases. This leaves the number of local states in local
state space as a free hyper parameter. In previous work it has been
shown that as you increase the number of local states, the accuracy of
MKS model increases [2], but as the number of local states increases,
the difference in accuracy decreases. Some work needs to be done in
order to find the practical number of local states that we will use.

Optimizing the Number of Local States
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Let's split the calibrate dataset into testing and training datasets.
The function ``train_test_split`` for the machine learning python module
```sklearn`` <http://scikit-learn.org/stable/>`__ provides a convenient
interface to do this. 80% of the dataset will be used for training and
the remaining 20% will be used for testing by setting ``test_size``
equal to 0.2. The state of the random number generator used to make the
split can be set using ``random_state``.

.. code:: python

    import sklearn
    from sklearn.cross_validation import train_test_split
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=3)
    print X_train.shape

.. parsed-literal::

    (200, 40, 40)


We are now going to calibrate the influence coefficients while varying
the number of local states from 2 up to 20. Each of these models will
then predict the evolution of the concentration fields. Mean square
error will be used to compared the results with the testing dataset to
evaluate how the MKS model's performance changes as we change the number
of local states.

First we need to import the class ``MKSRegressionModel`` from ``pymks``.
We will also use metrics from ``sklearn`` to calculate the mean squared
error.

.. code:: python

    from sklearn import metrics
    mse = metrics.mean_squared_error
    
    from pymks import MKSRegressionModel
    from pymks.bases import ContinuousIndicatorBasis
Next we will calibrate the influence coefficients while varying the
number of local states and compute the mean squared error. The following
demonstrates how to use Scikit-learn's ``GridSearchCV`` to optimize
``n_states`` as a hyperparameter. Of course, the best fit is always with
a larger value of ``n_states``. Increasing this parameter does not
overfit the data.

.. code:: python

    from sklearn.grid_search import GridSearchCV
    
    parameters_to_tune = {'n_states': np.arange(2, 11)}
    basis = ContinuousIndicatorBasis(2, [-1, 1])
    model = MKSRegressionModel(basis)
    scoring = metrics.make_scorer(lambda a, b: -mse(a, b))
    gs = GridSearchCV(model, parameters_to_tune, cv=5, scoring=scoring)
    gs.fit(X_train, y_train)



.. parsed-literal::

    GridSearchCV(cv=5,
           estimator=MKSRegressionModel(basis=<pymks.bases.continuous.ContinuousIndicatorBasis object at 0x7fa61aabfd10>,
              n_states=2),
           fit_params={}, iid=True, loss_func=None, n_jobs=1,
           param_grid={'n_states': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10])},
           pre_dispatch='2*n_jobs', refit=True, score_func=None,
           scoring=make_scorer(<lambda>), verbose=0)



.. code:: python

    print(gs.best_estimator_)
    print(gs.score(X_test, y_test))

.. parsed-literal::

    MKSRegressionModel(basis=<pymks.bases.continuous.ContinuousIndicatorBasis object at 0x7fa61a8b2fd0>,
              n_states=10)
    0.999999081316


.. code:: python

    from pymks.tools import draw_gridscores
    
    draw_gridscores(gs.grid_scores_)


.. image:: cahn_hilliard_files/cahn_hilliard_14_0.png


As expected the accuracy of the MKS model monotonically increases as we
increase n\_states, but accuracy doesn't improve significantly as
n\_states gets larger than signal digits.

In order to save on computation costs let's set calibrate the influence
coefficients with ``n_states`` equal to 6, but realize that if we need
slightly more accuracy the value can be increased.

.. code:: python

    MKSmodel = MKSRegressionModel(basis=ContinuousIndicatorBasis(6, [-1, 1]))
    MKSmodel.fit(X, y)
Here are the first 4 influence coefficients.

.. code:: python

    from pymks.tools import draw_coeff
    
    draw_coeff(MKSmodel.coeff[...,:4])


.. image:: cahn_hilliard_files/cahn_hilliard_18_0.png


Predict Microstructure Evolution
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

With the calibrated influence coefficients, we are ready to predict the
evolution of a concentration field. In order to do this, we need to have
the Cahn-Hilliard simulation and the MKS model start with the same
initial concentration ``phi0`` and evolve in time. In order to do the
Cahn-Hilliard simulation we need an instance of the class
``CahnHilliardSimulation``.

.. code:: python

    from pymks.datasets.cahnHilliardSimulation import CahnHilliardSimulation
    np.random.seed(191)
    
    phi0 = 2 * np.random.random((1, L, L)) - 1
    CHSim = CahnHilliardSimulation(dt=dt)
    phi = phi0.copy()
    phi_pred = phi0.copy()
    

In order to move forward in time, we need to feed the concentration back
into the Cahn-Hilliard simulation and the MKS model.

.. code:: python

    time_steps = 10
    
    for ii in range(time_steps):
        phi = CHSim.get_response(phi)
        phi_pred = MKSmodel.predict(phi_pred)
Let's take a look at the concentration fields.

.. code:: python

    from pymks.tools import draw_concentrations_compare
    
    draw_concentrations_compare(phi[0], phi_pred[0])


.. image:: cahn_hilliard_files/cahn_hilliard_24_0.png


The MKS model was able to capture the microstructure evolution with 6
local states.

Resizing the Coefficients to use on Larger Systems
--------------------------------------------------

Now let's try and predict a larger simulation by resizing the
coefficients and provide a larger initial concentratio field.

.. code:: python

    N = 3 * L
    MKSmodel.resize_coeff((N, N))
    
    phi0 = 2 * np.random.random((1, N, N)) - 1
    phi = phi0.copy()
    phi_pred = phi0.copy()
Once again we are going to march forward in time by feeding the
concentration fields back into the Cahn-Hilliard simulation and the MKS
model.

.. code:: python

    for ii in range(1000):
        phi = CHSim.get_response(phi)
        phi_pred = MKSmodel.predict(phi_pred)
Let's take a look at the results.

.. code:: python

    from pymks.tools import draw_concentrations_compare
    
    draw_concentrations_compare(phi[0], phi_pred[0])


.. image:: cahn_hilliard_files/cahn_hilliard_30_0.png


The MKS model with resized influence coefficients was able to reasonably
predict the structure evolution for a larger concentration field.

References
----------

[1] Ye X., Cheng X., The Fourier spectral method for the Cahn-Hilliard
equation. Applied Mathematics and Computation, 171, 1, 345-357, 2005.
`doi:10.1016/j.amc.2005.01.050 <http://dx.doi.org/10.1016/j.amc.2005.01.050>`__

[2] Fast T., Niezgoda S., Kalidindi S.R., A new framework for
computationally efficient structureâ€“structure evolution linkages to
facilitate high-fidelity scale bridging in multi-scale materials models
59, 2, 699-707, 2011.
`doi:10.1016/j.actamat.2010.10.008 <http://dx.doi.org/10.1016/j.actamat.2010.10.008>`__
